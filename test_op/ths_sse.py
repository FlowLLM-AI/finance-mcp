"""åŒèŠ±é¡ºæ•°æ®å…¨é‡çˆ¬å–è„šæœ¬ï¼ˆé›†æˆæœåŠ¡å¯åŠ¨ï¼‰

è¯¥è„šæœ¬æ•´åˆäº†æœåŠ¡å¯åŠ¨å’Œæ•°æ®çˆ¬å–åŠŸèƒ½ï¼Œæ— éœ€æ‰‹åŠ¨åœ¨ä¸¤ä¸ªç»ˆç«¯ä¸­åˆ†åˆ«å¯åŠ¨æœåŠ¡å’Œè„šæœ¬ã€‚
ä½¿ç”¨ FinanceMcpServiceRunner è‡ªåŠ¨å¯åŠ¨å’Œç®¡ç† finance-mcp æœåŠ¡ï¼Œå¹¶åœ¨åŒä¸€è¿›ç¨‹ä¸­æ‰§è¡Œæ‰¹é‡çˆ¬å–ä»»åŠ¡ã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
1. è‡ªåŠ¨å¯åŠ¨ finance-mcp æœåŠ¡ï¼ˆSSEæ¨¡å¼ï¼‰
2. æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼ˆåŸºäºè¿›åº¦æ–‡ä»¶ï¼‰
3. å¹¶å‘æ§åˆ¶ï¼ˆä¿¡å·é‡ï¼‰
4. æ‰¹é‡ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶
5. è‡ªåŠ¨æ–‡ä»¶åˆ‡åˆ†ï¼ˆ50MBï¼‰
6. æ™ºèƒ½ç­‰å¾…ç­–ç•¥ï¼ˆæ— è¾“å‡ºæ—¶é•¿ç­‰å¾…ï¼Œæ­£å¸¸æ—¶çŸ­ç­‰å¾…ï¼‰
"""

import asyncio
import json
import os
import random
import uuid
import pandas as pd
from datetime import datetime
from fastmcp.client.client import CallToolResult
from loguru import logger

from finance_mcp.core.utils.fastmcp_client import FastMcpClient
from finance_mcp.core.utils.service_runner import FinanceMcpServiceRunner

# --- æœåŠ¡é…ç½® ---
SERVICE_ARGS = [
    "finance-mcp",
    "config=default,ths",
    'disabled_flows=["tavily_search","mock_search","react_agent"]',
    "mcp.transport=sse",
]
HOST = "localhost"
PORT = 8050
os.environ.setdefault("NO_PROXY", "*")
# --- æ•°æ®é…ç½® ---
CSV_PATH = "tushare_stock_basic_20251226104714.csv"
BASE_CACHE_DIR = "tool_cache"
PROGRESS_DIR = os.path.join(BASE_CACHE_DIR, "progress")
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
SAVE_BATCH_SIZE = 1               # æ¯1ä¸ªä¿å­˜ä¸€æ¬¡
MAX_CONCURRENCY = 5              # æœ€å¤§å¹¶å‘æ•°ï¼ˆä¿¡å·é‡æ§åˆ¶ï¼‰
PROGRESS_REPORT_INTERVAL = 10    # æ¯å¤„ç†Næ¡è®°å½•æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦æ±‡æ€»
MIN_WAIT_ON_EMPTY = 60            # æ— è¾“å‡ºæ—¶æœ€å°ç­‰å¾…ç§’æ•°
MAX_WAIT_ON_EMPTY = 120           # æ— è¾“å‡ºæ—¶æœ€å¤§ç­‰å¾…ç§’æ•°
NORMAL_WAIT_SECONDS = 3           # æ­£å¸¸è¯·æ±‚é—´éš”ç§’æ•°

# æ— æ•ˆç»“æœæ ‡è¯†ï¼ˆè¿™äº›ç»“æœéœ€è¦é‡æ–°çˆ¬å–ï¼‰
INVALID_RESULTS = [
    "No relevant content found matching the query.",
    "æœªæ‰¾åˆ°ä¸æŸ¥è¯¢åŒ¹é…çš„ç›¸å…³å†…å®¹",
]

# é’ˆå¯¹æ¯ä¸ªé¡µé¢ç»“æ„è®¾è®¡çš„å…¨é‡æå– Query
TOOLS_CONFIG = [
    # ("crawl_ths_company", "æå–å…¬å¸çš„å®Œæ•´èµ„æ–™ï¼š1.åŸºæœ¬ä¿¡æ¯ï¼ˆè¡Œä¸šã€äº§å“ã€ä¸»è¥ã€åŠå…¬åœ°å€ï¼‰ï¼›2.é«˜ç®¡ä»‹ç»ï¼ˆæ‰€æœ‰é«˜ç®¡çš„å§“åã€èŒåŠ¡ã€è–ªèµ„ã€è¯¦ç»†ä¸ªäººç®€å†ï¼‰ï¼›3.å‘è¡Œç›¸å…³ï¼ˆä¸Šå¸‚æ—¥æœŸã€é¦–æ—¥è¡¨ç°ã€å‹Ÿèµ„é¢ï¼‰ï¼›4.æ‰€æœ‰å‚æ§è‚¡å…¬å¸çš„åç§°ã€æŒè‚¡æ¯”ä¾‹ã€ä¸šåŠ¡ã€ç›ˆäºæƒ…å†µã€‚"),
    # ("crawl_ths_holder", "æå–è‚¡ä¸œç ”ç©¶å…¨é‡æ•°æ®ï¼š1.å†å¹´è‚¡ä¸œäººæ•°åŠæˆ·å‡æŒè‚¡æ•°ï¼›2.å‰åå¤§è‚¡ä¸œåŠæµé€šè‚¡ä¸œåå•ï¼ˆå«æŒè‚¡æ•°ã€æ€§è´¨ã€å˜åŠ¨æƒ…å†µï¼‰ï¼›3.å®é™…æ§åˆ¶äººè¯¦æƒ…åŠæ§è‚¡å±‚çº§å…³ç³»æè¿°ï¼›4.è‚¡æƒè´¨æŠ¼ã€å†»ç»“çš„è¯¦ç»†æ˜ç»†è¡¨ã€‚"),
    # ("crawl_ths_operate", "æå–ç»è¥åˆ†ææ•°æ®ï¼š1.ä¸»è¥æ„æˆåˆ†æè¡¨ï¼ˆæŒ‰è¡Œä¸šã€äº§å“ã€åŒºåŸŸåˆ’åˆ†çš„è¥ä¸šæ”¶å…¥ã€åˆ©æ¶¦ã€æ¯›åˆ©ç‡åŠåŒæ¯”å˜åŒ–ï¼‰ï¼›2.ç»è¥è¯„è¿°ï¼ˆå…¬å¸å¯¹ä¸šåŠ¡ã€æ ¸å¿ƒç«äº‰åŠ›çš„è¯¦ç»†è‡ªæˆ‘è¯„ä¼°ï¼‰ã€‚"),
    # ("crawl_ths_equity", "æå–è‚¡æœ¬ç»“æ„ä¿¡æ¯ï¼š1.å†æ¬¡è‚¡æœ¬å˜åŠ¨åŸå› ã€æ—¥æœŸåŠå˜åŠ¨åçš„æ€»è‚¡æœ¬ï¼›2.é™å”®è‚¡ä»½è§£ç¦çš„æ—¶é—´è¡¨ã€è§£ç¦æ•°é‡åŠå æ€»è‚¡æœ¬æ¯”ä¾‹ã€‚"),
    # ("crawl_ths_capital", "æå–èµ„æœ¬è¿ä½œè¯¦æƒ…ï¼š1.èµ„äº§é‡ç»„ã€æ”¶è´­ã€åˆå¹¶çš„è¯¦ç»†å†å²è®°å½•ï¼›2.å¯¹å¤–æŠ•èµ„æ˜ç»†åŠè¿›å±•æƒ…å†µã€‚"),
    # ("crawl_ths_worth", "æå–ç›ˆåˆ©é¢„æµ‹ä¿¡æ¯ï¼š1.å„æœºæ„æœ€æ–°è¯„çº§æ±‡æ€»ï¼ˆä¹°å…¥/å¢æŒæ¬¡æ•°ï¼‰ï¼›2.æœªæ¥ä¸‰å¹´çš„è¥æ”¶é¢„æµ‹ã€å‡€åˆ©æ¶¦é¢„æµ‹åŠEPSé¢„æµ‹å‡å€¼ã€‚"),
    # ("crawl_ths_news", "æå–æœ€æ–°æ–°é—»å…¬å‘Šï¼š1.å…¬å¸æœ€æ–°é‡è¦å…¬å‘Šæ ‡é¢˜åŠæ—¥æœŸï¼›2.åª’ä½“æŠ¥é“çš„æ–°é—»æ‘˜è¦åŠèˆ†æƒ…è¯„ä»·ã€‚"),
    # ("crawl_ths_concept", "æå–æ‰€æœ‰æ¦‚å¿µé¢˜æï¼šåˆ—å‡ºå…¬å¸æ‰€å±çš„æ‰€æœ‰æ¦‚å¿µæ¿å—ï¼Œå¹¶è¯¦ç»†æå–æ¯ä¸ªæ¦‚å¿µå¯¹åº”çš„å…·ä½“å…¥é€‰ç†ç”±å’Œä¸šåŠ¡å…³è”æ€§ã€‚"),
    # ("crawl_ths_position", "æå–ä¸»åŠ›æŒä»“æƒ…å†µï¼š1.å„ç±»æœºæ„ï¼ˆåŸºé‡‘ã€ä¿é™©ã€QFIIç­‰ï¼‰æŒä»“æ€»æ•°åŠå æ¯”ï¼›2.å‰åå¤§å…·ä½“æœºæ„æŒä»“åå•åŠå˜åŠ¨ã€‚"),
    ("crawl_ths_finance", "æå–è´¢åŠ¡åˆ†æè¯¦æƒ…ï¼š1.ä¸»è¦è´¢åŠ¡æŒ‡æ ‡ï¼ˆç›ˆåˆ©ã€æˆé•¿ã€å¿å€ºç­‰ï¼‰ï¼›2.èµ„äº§è´Ÿå€ºè¡¨ã€åˆ©æ¶¦è¡¨ã€ç°é‡‘æµé‡è¡¨çš„æ ¸å¿ƒç§‘ç›®åŠå®¡è®¡æ„è§ã€‚"),
    # ("crawl_ths_bonus", "æå–åˆ†çº¢èèµ„è®°å½•ï¼š1.å†å¹´ç°é‡‘åˆ†çº¢ã€é€è½¬è‚¡ä»½æ–¹æ¡ˆåŠå®æ–½æ—¥æœŸï¼›2.å†æ¬¡å¢å‘ã€é…è‚¡ç­‰èèµ„è¯¦æƒ…ã€‚"),
    ("crawl_ths_event", "æå–å…¬å¸å¤§äº‹è®°å½•ï¼š1.è‚¡ä¸œåŠé«˜ç®¡æŒè‚¡å˜åŠ¨æ˜ç»†ï¼›2.å¯¹å¤–æ‹…ä¿è®°å½•ã€è¿è§„å¤„ç†ã€æœºæ„è°ƒç ”åŠæŠ•èµ„è€…äº’åŠ¨è®°å½•ã€‚"),
    # ("crawl_ths_field", "æå–è¡Œä¸šå¯¹æ¯”æ•°æ®ï¼š1.å…¬å¸åœ¨æ‰€å±è¡Œä¸šå†…çš„è§„æ¨¡ã€æˆé•¿ã€ç›ˆåˆ©å„é¡¹æ’åï¼›2.ä¸è¡Œä¸šå‡å€¼åŠåŒç±»ç«å“çš„å…³é”®è´¢åŠ¡æŒ‡æ ‡å¯¹æ¯”ã€‚")
]

# åˆ›å»ºå¿…è¦çš„ç›®å½•
if not os.path.exists(BASE_CACHE_DIR):
    os.makedirs(BASE_CACHE_DIR, exist_ok=True)
os.makedirs(PROGRESS_DIR, exist_ok=True)


class BatchResultSaver:
    """æ‰¹é‡ç»“æœä¿å­˜å™¨ï¼Œæ”¯æŒè‡ªåŠ¨æ–‡ä»¶åˆ‡åˆ†"""
    
    def __init__(self, tool_name: str):
        self.tool_name = tool_name
        self.buffer = []
        self.file_index = 1
        
    def _get_file_path(self) -> str:
        """è·å–å½“å‰ä¿å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(BASE_CACHE_DIR, f"{self.tool_name}_{self.file_index:02d}.json")

    def add_record(self, tool_args: dict, result_text: str):
        """æ·»åŠ ä¸€æ¡è®°å½•åˆ°ç¼“å†²åŒº"""
        now = datetime.now()
        record = {
            "_id": str(uuid.uuid4()),
            "cache_key": f"{self.tool_name}::{json.dumps(tool_args, ensure_ascii=False)}",
            "created_at": now.isoformat(),
            "metadata": {"task_id": "comprehensive_crawl", "timestamp": now.isoformat()},
            "tool_args": tool_args,
            "tool_name": self.tool_name,
            "tool_result": result_text,
            "updated_at": now.isoformat()
        }
        self.buffer.append(record)
        if len(self.buffer) >= SAVE_BATCH_SIZE:
            self.flush()

    def flush(self):
        """å°†ç¼“å†²åŒºæ•°æ®å†™å…¥æ–‡ä»¶"""
        if not self.buffer:
            return
            
        file_path = self._get_file_path()
        
        # è‡ªåŠ¨åˆ‡åˆ†ï¼šå¦‚æœæ–‡ä»¶è¶…è¿‡ 50MBï¼Œåˆ‡æ¢åˆ°æ–°æ–‡ä»¶
        if os.path.exists(file_path) and os.path.getsize(file_path) > MAX_FILE_SIZE:
            self.file_index += 1
            file_path = self._get_file_path()

        # è¯»å–ç°æœ‰æ•°æ®
        data = []
        if os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8") as f:
                try:
                    data = json.load(f)
                except:
                    data = []
        
        # è¿½åŠ æ–°æ•°æ®å¹¶ä¿å­˜
        data.extend(self.buffer)
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜æ–‡ä»¶: {file_path} | æ€»è®°å½•æ•°: {len(data)}")
        self.buffer = []


class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨ï¼Œæ”¯æŒæ–­ç‚¹ç»§ç»­"""
    
    def __init__(self, tool_name: str):
        self.tool_name = tool_name
        self.progress_file = os.path.join(PROGRESS_DIR, f"{tool_name}_progress.json")
        self.completed_codes = set()
        self.time_records = {}  # è®°å½•æ¯ä¸ªcodeçš„å¤„ç†æ—¶é—´
        self.load_progress()
    
    def load_progress(self):
        """åŠ è½½å·²æœ‰çš„è¿›åº¦æ•°æ®"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.completed_codes = set(data.get("completed_codes", []))
                    self.time_records = data.get("time_records", {})
                logger.info(f"åŠ è½½è¿›åº¦æ–‡ä»¶: {self.progress_file}ï¼Œå·²å®Œæˆ {len(self.completed_codes)} ä¸ª")
            except Exception as e:
                logger.warning(f"åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥ {self.progress_file}: {e}")
                self.completed_codes = set()
                self.time_records = {}
    
    def save_progress(self):
        """ä¿å­˜å½“å‰è¿›åº¦"""
        try:
            data = {
                "completed_codes": list(self.completed_codes),
                "time_records": self.time_records
            }
            with open(self.progress_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜è¿›åº¦æ–‡ä»¶å¤±è´¥ {self.progress_file}: {e}")
    
    def mark_completed(self, code: str, elapsed_time: float | None = None):
        """æ ‡è®°æŸä¸ªä»£ç ä¸ºå·²å®Œæˆ"""
        self.completed_codes.add(code)
        if elapsed_time is not None:
            self.time_records[code] = {
                "elapsed_seconds": round(elapsed_time, 2),
                "completed_at": datetime.now().isoformat()
            }
    
    def is_completed(self, code: str) -> bool:
        """æ£€æŸ¥æŸä¸ªä»£ç æ˜¯å¦å·²å®Œæˆ"""
        return code in self.completed_codes
    
    def get_remaining_codes(self, all_codes: list) -> list:
        """è·å–å‰©ä½™éœ€è¦å¤„ç†çš„ä»£ç åˆ—è¡¨"""
        return [code for code in all_codes if not self.is_completed(code)]


MAX_RETRIES = 1  # æœ€å¤§é‡è¯•æ¬¡æ•°


# å…¨å±€è®¡æ•°å™¨ï¼Œç”¨äºè¿½è¸ªå¤„ç†è¿›åº¦
class GlobalProgressCounter:
    def __init__(self):
        self.processed_count = 0
        self.lock = asyncio.Lock()
    
    async def increment(self):
        async with self.lock:
            self.processed_count += 1
            return self.processed_count

global_counter = GlobalProgressCounter()


async def process_single_stock(
    client: FastMcpClient,
    tool_name: str,
    code: str,
    deep_query: str,
    saver: BatchResultSaver,
    progress_tracker: ProgressTracker,
    semaphore: asyncio.Semaphore,
    index: int,
    total: int,
    total_all_tools: int = 0
):
    """å¤„ç†å•ä¸ªè‚¡ç¥¨çš„å¼‚æ­¥ä»»åŠ¡"""
    async with semaphore:  # ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        args = {"code": code, "query": deep_query}
        
        for attempt in range(1, MAX_RETRIES + 1):
            start_time = datetime.now()
            
            try:
                logger.info(f"[{index}/{total}] è°ƒç”¨ {tool_name} å¤„ç†è‚¡ç¥¨ {code} (å°è¯• {attempt}/{MAX_RETRIES})")
                result: CallToolResult = await client.call_tool(tool_name, args)
                elapsed_time = (datetime.now() - start_time).total_seconds()
                
                if not result.is_error:
                    content = result.content[0].text if result.content else ""
                    
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºã€å¤ªçŸ­æˆ–æ˜¯æ— æ•ˆç»“æœ
                    is_invalid = (
                        not content 
                        or len(content.strip()) < 10 
                        or content.strip() in INVALID_RESULTS
                    )
                    
                    if is_invalid:
                        if attempt < MAX_RETRIES:
                            wait_seconds = random.randint(MIN_WAIT_ON_EMPTY, MAX_WAIT_ON_EMPTY)
                            logger.warning(
                                f"å·¥å…· {tool_name} å¤„ç† {code} æ— æ•ˆç»“æœ: '{content[:50]}...', "
                                f"è€—æ—¶ {elapsed_time:.2f}ç§’, ç­‰å¾… {wait_seconds} ç§’åé‡è¯• ({attempt}/{MAX_RETRIES})..."
                            )
                            await asyncio.sleep(wait_seconds)
                            continue  # é‡è¯•
                        else:
                            logger.error(
                                f"å·¥å…· {tool_name} å¤„ç† {code} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {MAX_RETRIES}, "
                                f"ä»ä¸ºæ— æ•ˆç»“æœ: '{content[:50]}...', è·³è¿‡"
                            )
                            return  # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒ
                    else:
                        # æœ‰æ­£å¸¸è¾“å‡ºï¼Œä¿å­˜è®°å½•
                        saver.add_record(args, content)
                        # æ ‡è®°è¯¥ä»£ç å·²å®Œæˆï¼Œè®°å½•è€—æ—¶
                        progress_tracker.mark_completed(code, elapsed_time)
                        # å®šæœŸä¿å­˜è¿›åº¦
                        if index % SAVE_BATCH_SIZE == 0:
                            progress_tracker.save_progress()
                        
                        # æ›´æ–°å…¨å±€è®¡æ•°å¹¶æ˜¾ç¤ºå‰©ä½™æ•°é‡
                        global_processed = await global_counter.increment()
                        remaining_this_tool = total - index
                        remaining_all = total_all_tools - global_processed
                        
                        # æ­£å¸¸ç­‰å¾…é—´éš”ï¼Œæ˜¾ç¤ºå‰©ä½™æ•°é‡
                        logger.info(
                            f"âœ“ æˆåŠŸå¤„ç† {code}, è€—æ—¶ {elapsed_time:.2f}ç§’ | "
                            f"å½“å‰å·¥å…·å‰©ä½™: {remaining_this_tool}/{total} | "
                            f"æ€»å‰©ä½™: {remaining_all}"
                        )
                        
                        # æ¯éš”ä¸€å®šæ•°é‡æ˜¾ç¤ºè¯¦ç»†è¿›åº¦æ±‡æ€»
                        if global_processed % PROGRESS_REPORT_INTERVAL == 0:
                            logger.info(
                                f"\n{'â”€'*40}\n"
                                f"ğŸ“Š è¿›åº¦æ±‡æ€»: å·²å¤„ç† {global_processed} æ¡, æ€»å‰©ä½™ {remaining_all} æ¡\n"
                                f"{'â”€'*40}"
                            )
                        
                        await asyncio.sleep(NORMAL_WAIT_SECONDS)
                        return  # æˆåŠŸï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                else:
                    error_msg = result.content[0].text[:100] if result.content else "No error message"
                    if attempt < MAX_RETRIES:
                        wait_seconds = random.randint(MIN_WAIT_ON_EMPTY, MAX_WAIT_ON_EMPTY)
                        logger.warning(f"âœ— é”™è¯¯ {code}: {error_msg}, ç­‰å¾… {wait_seconds} ç§’åé‡è¯• ({attempt}/{MAX_RETRIES})...")
                        await asyncio.sleep(wait_seconds)
                        continue  # é‡è¯•
                    else:
                        logger.error(f"âœ— é”™è¯¯ {code}: {error_msg}, è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {MAX_RETRIES}, è·³è¿‡")
                        return
                    
            except Exception as e:
                import traceback
                elapsed_time = (datetime.now() - start_time).total_seconds()
                error_str = str(e)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯â€œä¸å½“å†…å®¹â€é”™è¯¯ï¼Œè¿™ç§æƒ…å†µä¸éœ€è¦é‡è¯•
                if "inappropriate content" in error_str:
                    logger.warning(f"âš  {code}: è¿”å›ä¸å½“å†…å®¹é”™è¯¯ï¼Œè·³è¿‡ä¸é‡è¯•")
                    return
                
                if attempt < MAX_RETRIES:
                    wait_seconds = random.randint(MIN_WAIT_ON_EMPTY, MAX_WAIT_ON_EMPTY)
                    logger.warning(f"âœ— å¤±è´¥ {code}: {e}, ç­‰å¾… {wait_seconds} ç§’åé‡è¯• ({attempt}/{MAX_RETRIES})...")
                    await asyncio.sleep(wait_seconds)
                    continue  # é‡è¯•
                else:
                    logger.error(f"âœ— å¤±è´¥ {code}: {e}, è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {MAX_RETRIES}, è·³è¿‡")
                    logger.error(f"Traceback: {traceback.format_exc()}")
                    return


async def run_crawl_task():
    """æ‰§è¡Œçˆ¬å–ä»»åŠ¡çš„ä¸»å‡½æ•°"""
    # è¯»å–è‚¡ç¥¨ä»£ç åˆ—è¡¨
    logger.info(f"è¯»å–è‚¡ç¥¨ä»£ç åˆ—è¡¨: {CSV_PATH}")
    df = pd.read_csv(CSV_PATH, dtype={'symbol': str})
    stock_codes = df['symbol'].dropna().apply(lambda x: str(x).zfill(6)).tolist()
    logger.info(f"å…±åŠ è½½ {len(stock_codes)} ä¸ªè‚¡ç¥¨ä»£ç ")
    
    # MCP å®¢æˆ·ç«¯é…ç½®
    mcp_config = {"type": "sse", "url": f"http://{HOST}:{PORT}/sse"}
    
    # ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)

    # ã€ç¬¬äºŒæ­¥ã€‘æ±‡æ€»ç»Ÿè®¡å„å·¥å…·å¾…çˆ¬å–æ•°é‡
    logger.info(f"\n{'='*60}")
    logger.info("ã€ç¬¬äºŒæ­¥ã€‘ç»Ÿè®¡å„å·¥å…·å¾…çˆ¬å–æ•°é‡...")
    logger.info(f"{'='*60}")
    
    total_tasks = 0
    tool_stats = []
    for tool_name, deep_query in TOOLS_CONFIG:
        progress_tracker = ProgressTracker(tool_name)
        remaining_codes = progress_tracker.get_remaining_codes(stock_codes)
        total_remaining = len(remaining_codes)
        completed_count = len(stock_codes) - total_remaining
        total_tasks += total_remaining
        tool_stats.append((tool_name, completed_count, total_remaining))
        # åªåˆ—å‡ºè¿˜éœ€è¦çˆ¬å–çš„å·¥å…·
        if total_remaining > 0:
            logger.info(
                f"  {tool_name}: å·²å®Œæˆ {completed_count}, å¾…çˆ¬å– {total_remaining}"
            )
    
    logger.info(f"{'='*60}")
    logger.info(f"æ±‡æ€»: å…± {len(TOOLS_CONFIG)} ä¸ªå·¥å…·, æ€»è®¡å¾…çˆ¬å– {total_tasks} æ¡è®°å½•")
    logger.info(f"{'='*60}\n")
    
    if total_tasks == 0:
        logger.info("æ‰€æœ‰å·¥å…·å·²å®Œæˆçˆ¬å–ï¼Œæ— éœ€ç»§ç»­")
        return
    # return
    
    # é‡ç½®å…¨å±€è®¡æ•°å™¨
    global global_counter
    global_counter = GlobalProgressCounter()
    
    # ã€ç¬¬ä¸‰æ­¥ã€‘å¼€å§‹çˆ¬å–ä»»åŠ¡
    logger.info(f"\n{'='*60}")
    logger.info("ã€ç¬¬ä¸‰æ­¥ã€‘å¼€å§‹çˆ¬å–ä»»åŠ¡...")
    logger.info(f"{'='*60}\n")
    
    async with FastMcpClient(name="full-info-crawler", config=mcp_config) as client:
        # å¤–å±‚å¾ªç¯ï¼šéå†æ‰€æœ‰å·¥å…·
        for tool_name, deep_query in TOOLS_CONFIG:
            logger.info(f"\n{'-'*60}")
            logger.info(f"å¼€å§‹çˆ¬å–å·¥å…·: {tool_name}")
            logger.info(f"æŸ¥è¯¢å†…å®¹: {deep_query}")
            logger.info(f"{'-'*60}")
            
            saver = BatchResultSaver(tool_name)
            progress_tracker = ProgressTracker(tool_name)
            
            # è·å–å‰©ä½™éœ€è¦å¤„ç†çš„è‚¡ç¥¨ä»£ç 
            remaining_codes = progress_tracker.get_remaining_codes(stock_codes)
            total_remaining = len(remaining_codes)
            completed_count = len(stock_codes) - total_remaining
            
            logger.info(
                f"å·¥å…· {tool_name}: æ€»è®¡ {len(stock_codes)} ä¸ªè‚¡ç¥¨, "
                f"å·²å®Œæˆ {completed_count} ä¸ª, å‰©ä½™ {total_remaining} ä¸ª"
            )
            
            if total_remaining == 0:
                logger.info(f"å·¥å…· {tool_name} æ‰€æœ‰è‚¡ç¥¨å·²å¤„ç†å®Œæˆï¼Œè·³è¿‡")
                continue
            
            # åˆ›å»ºæ‰€æœ‰å¹¶å‘ä»»åŠ¡
            tasks = []
            for i, code in enumerate(remaining_codes, start=1):
                task = process_single_stock(
                    client=client,
                    tool_name=tool_name,
                    code=code,
                    deep_query=deep_query,
                    saver=saver,
                    progress_tracker=progress_tracker,
                    semaphore=semaphore,
                    index=i,
                    total=total_remaining,
                    total_all_tools=total_tasks
                )
                tasks.append(task)
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼Œä¿¡å·é‡æ§åˆ¶æœ€å¤š MAX_CONCURRENCY ä¸ªåŒæ—¶è¿è¡Œ
            logger.info(f"å¯åŠ¨ {len(tasks)} ä¸ªå¹¶å‘ä»»åŠ¡ï¼Œæœ€å¤§å¹¶å‘æ•°: {MAX_CONCURRENCY}")
            await asyncio.gather(*tasks)
            
            # ä¿å­˜æœ€åçš„è¿›åº¦
            progress_tracker.save_progress()
            saver.flush()
            logger.info(f"\n{'='*80}")
            logger.info(f"âœ“ å·¥å…· {tool_name} å®Œæˆ!")
            logger.info(f"{'='*80}\n")


def main():
    """ä¸»å‡½æ•°ï¼šå¯åŠ¨æœåŠ¡å¹¶è¿è¡Œçˆ¬å–ä»»åŠ¡"""
    logger.info("="*80)
    logger.info("å¼€å§‹å¯åŠ¨ finance-mcp æœåŠ¡...")
    logger.info(f"æœåŠ¡å‚æ•°: {SERVICE_ARGS}")
    logger.info(f"ç›‘å¬åœ°å€: {HOST}:{PORT}")
    logger.info("="*80)
    
    # ä½¿ç”¨ FinanceMcpServiceRunner å¯åŠ¨æœåŠ¡
    with FinanceMcpServiceRunner(
        SERVICE_ARGS,
        host=HOST,
        port=PORT,
    ) as service:
        logger.info(f"âœ“ æœåŠ¡å·²å¯åŠ¨ï¼Œç›‘å¬ç«¯å£: {service.port}")
        logger.info("å¼€å§‹æ‰§è¡Œçˆ¬å–ä»»åŠ¡...\n")
        
        # è¿è¡Œçˆ¬å–ä»»åŠ¡
        asyncio.run(run_crawl_task())
        
        logger.info("\n" + "="*80)
        logger.info("âœ“ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ")
        logger.info("="*80)


if __name__ == "__main__":
    main()
